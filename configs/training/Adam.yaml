# @package training
upper_iterations: 100

optimizer:
  upper:
    name: 'core.optimizers.AdamOptimizer'
    lr : 0.01
    momentum: 0.
    nesterov: False
  lower:
    name: 'core.optimizers.AdamOptimizer'
    lr : 0.01
    momentum: 0.
    nesterov: False
scheduler:
  upper:
    name: 'torch.optim.lr_scheduler.CosineAnnealingLR'
  lower:
    name: 'torch.optim.lr_scheduler.CosineAnnealingLR'
