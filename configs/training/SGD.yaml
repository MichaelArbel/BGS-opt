# @package training
upper_iterations: 100

optimizer:
  upper:
    name: 'torch.optim.SGD'
    lr : 0.01
    momentum: 0.
    nesterov: False
  lower:
    name: 'torch.optim.SGD'
    lr : 0.01
    momentum: 0.
    nesterov: False
scheduler:
  upper:
    name: 'torch.optim.lr_scheduler.CosineAnnealingLR'
    T_max: 1000
    use_scheduler: False
  lower:
    name: 'torch.optim.lr_scheduler.CosineAnnealingLR'
    T_max: 1000
    use_scheduler: False